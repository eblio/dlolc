{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, classes, dataset='train', image_size=64):\n",
    "\n",
    "    num_images = 0\n",
    "    for i in range(len(classes)):\n",
    "        dirs = sorted(os.listdir(data_path + dataset + '/' + classes[i]))\n",
    "        num_images += len(dirs)\n",
    "                                \n",
    "    x = np.zeros((num_images, image_size, image_size, 3))\n",
    "    y = np.zeros((num_images, 1))\n",
    "    \n",
    "    current_index = 0\n",
    "    \n",
    "    # Parcours des différents répertoires pour collecter les images\n",
    "    for idx_class in range(len(classes)):\n",
    "        dirs = sorted(os.listdir(data_path + dataset + '/' + classes[idx_class]))\n",
    "        num_images += len(dirs)\n",
    "    \n",
    "        # Chargement des images, \n",
    "        for idx_img in range(len(dirs)):\n",
    "            item = dirs[idx_img]\n",
    "            if os.path.isfile(data_path + dataset + '/' + classes[idx_class] + '/' + item):\n",
    "                # Ouverture de l'image\n",
    "                img = Image.open(data_path + dataset + '/' + classes[idx_class] + '/' + item)\n",
    "                # Redimensionnement de l'image et écriture dans la variable de retour x \n",
    "                img = img.resize((image_size,image_size))\n",
    "                x[current_index] = np.asarray(img)\n",
    "                # Écriture du label associé dans la variable de retour y\n",
    "                y[current_index] = idx_class\n",
    "                current_index += 1\n",
    "                \n",
    "    return x, y\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/\"\n",
    "labels = [\"Chogath\", \"Ezreal\", \"Lucian\", \"Malzahar\", \"Morgana\", \"Poppy\", \"Reksai\", \"Senna\", \"Syndra\", \"Teemo\"]\n",
    "\n",
    "#x_train, y_train = load_data(path, labels, dataset='train', image_size=600)\n",
    "#print(x_train.shape, y_train.shape)\n",
    "\n",
    "#x_val, y_val = load_data(path, labels, dataset='validation', image_size=600)\n",
    "#print(x_val.shape, y_val.shape)\n",
    "\n",
    "#x_test, y_test = load_data(path, labels, dataset='test', image_size=600)\n",
    "#print(x_test.shape, y_test.shape)\n",
    "\n",
    "#class_num = y_test.shape[1]\n",
    "\n",
    "#plt.figure(figsize=(12, 12))\n",
    "#shuffle_indices = np.random.permutation(9)\n",
    "#for i in range(0, 9):\n",
    "#    plt.subplot(3, 3, i+1)\n",
    "#    image = x_train[shuffle_indices[i]]\n",
    "#    plt.title(labels[int(y_train[shuffle_indices[i]])])\n",
    "#    plt.imshow(image/255)\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "# load and iterate training dataset\n",
    "train_it = datagen.flow_from_directory('../data/train/', class_mode='sparse', batch_size=64)\n",
    "# load and iterate validation dataset\n",
    "val_it = datagen.flow_from_directory('../data/validation/', class_mode='sparse', batch_size=64)\n",
    "# load and iterate test dataset\n",
    "test_it = datagen.flow_from_directory('../data/test/', class_mode='sparse', batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(600,600,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "optimizer = 'adam'\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size_train = train_it.n//train_it.batch_size\n",
    "step_size_valid = val_it.n//val_it.batch_size\n",
    "\n",
    "model.fit_generator(generator=train_it,\n",
    "                   steps_per_epoch = step_size_train,\n",
    "                   epochs = 10,\n",
    "                   verbose = 1,\n",
    "                   validation_data = val_it,\n",
    "                   validation_steps = step_size_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ]
}